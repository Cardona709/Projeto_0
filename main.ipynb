{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup libraries and Gemini API Key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants # to use the constants defined in the constants.py file\n",
    "import os\n",
    "from google import genai # to use the GenAI API\n",
    "from pydantic import BaseMode   l # to enforce a .JSON output schema to the model\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini Models\n",
    "|a|b|c|\n",
    "|-------|-------|-------|\n",
    "|Gemini 2.5 Pro Experimental|**gemini-2.5-pro-exp-03-25**|Audio, images, videos, and text \tText \tEnhanced thinking and reasoning, multimodal understanding, advanced coding, and more|\n",
    "|Gemini 2.0 Flash| **gemini-2.0-flash**\t|Audio, images, videos, and text \tText, images (experimental), and audio (coming soon) \tNext generation features, speed, thinking, realtime streaming, and multimodal generation|\n",
    "|Gemini 2.0 Flash-Lite| **gemini-2.0-flash-lite**|\tAudio, images, videos, and text \tText \tCost efficiency and low latency|\n",
    "|Gemini 1.5 Flash| **gemini-1.5-flash**|\tAudio, images, videos, and text \tText \tFast and versatile performance across a diverse variety of tasks|\n",
    "|Gemini 1.5 Flash-8B| **gemini-1.5-flash-8b** |\tAudio, images, videos, and text  Text \tHigh volume and lower intelligence tasks|\n",
    "|Gemini 1.5 Pro| **gemini-1.5-pro** |\tAudio, images, videos, and text \tText  Complex reasoning tasks requiring more intelligence|\n",
    "|Gemini Embedding| **gemini-embedding-exp** |\tText \tText embeddings \tMeasuring the relatedness of text strings|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and retrieve the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_path = 'Testes-main/Images'\n",
    "\n",
    "image_paths = []\n",
    "\n",
    "for filename in os.listdir(image_dir_path):\n",
    "    if (filename.endswith(\".jpg\") or filename.endswith(\".png\")) and filename.startswith(\"fire\"):\n",
    "        image_path = os.path.join(image_dir_path, filename)\n",
    "        image_paths.append(image_path)\n",
    "        \n",
    "print(f\"Found {len(image_paths)} images in {image_dir_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths.sort()\n",
    "titles=[\n",
    "    os.path.basename(image_path)\n",
    "    for image_path in image_paths[:len(image_paths)]\n",
    "    ]               \n",
    "\n",
    "images = [\n",
    "    cv2.imread(str(image_path))\n",
    "    for image_path in image_paths[:len(image_paths)]\n",
    "    ]\n",
    "\n",
    "sv.plot_images_grid(images=images, titles=titles,grid_size=(3,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO model selection & object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = YOLO('yolo11x.pt')\n",
    "model12 = YOLO('yolo12x.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "titles = []\n",
    "for image_path in image_paths:\n",
    "    test_iamge_path = cv2.imread(str(image_path))\n",
    "    test_iamge_path_resized = cv2.resize(test_iamge_path, (640, 640))\n",
    "    \n",
    "    results.append(model11(test_iamge_path))\n",
    "    titles.append(os.path.basename(image_path)+\" - yolo11\")\n",
    "    results.append(model11(test_iamge_path_resized))\n",
    "    titles.append(os.path.basename(image_path)+\" - yolo11 resized\")\n",
    "    results.append(model12(test_iamge_path))\n",
    "    titles.append(os.path.basename(image_path)+\" - yolo12\")\n",
    "    \n",
    "    \n",
    "    results.append(model12(test_iamge_path_resized))\n",
    "    titles.append(os.path.basename(image_path)+\" - yolo12 resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_images = []\n",
    "for i in range(0, len(results), 4):\n",
    "    comparison_images.append(results[i][0].plot())\n",
    "    comparison_images.append(results[i + 1][0].plot())\n",
    "    comparison_images.append(results[i + 2][0].plot())\n",
    "    comparison_images.append(results[i + 3][0].plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.plot_images_grid(images=comparison_images, titles=titles, grid_size=(5, 4), size=(50, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the image to analyze and extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model11\n",
    "test_iamge_path = image_paths[0]\n",
    "original_image = cv2.imread(test_iamge_path)\n",
    "test_result = results[0]\n",
    "test_image = test_result[0].plot()\n",
    "sv.plot_image(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the origin (0, 0) point at the center of the image\n",
    "origin = (original_image.shape[1] // 2, original_image.shape[0] // 2)\n",
    "\n",
    "# Draw the X-axis\n",
    "cv2.line(original_image, (0, origin[1]), (original_image.shape[1], origin[1]), (0, 255, 5), 2)\n",
    "\n",
    "# Draw the Y-axis\n",
    "cv2.line(original_image, (origin[0], 0), (origin[0], original_image.shape[0]), (0, 255, 5), 2)\n",
    "\n",
    "# Add labels for the axes\n",
    "cv2.putText(original_image, 'X', (original_image.shape[1] - 50, origin[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 5), 2)\n",
    "cv2.putText(original_image, 'Y', (origin[0] + 10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 5), 2)\n",
    "cv2.putText(original_image, '(0,0)', (origin[0] + 10, origin[1] + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 5), 2)\n",
    "\n",
    "# Display the image with the referential\n",
    "sv.plot_image(original_image, size=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [model.names.get(cls.item()) for cls in test_result[0].boxes.cls.int()]  \n",
    "        \n",
    "unique_labels = set(labels)\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "\n",
    "probabilities = test_result[0].boxes.conf\n",
    "print(\"Probabilities:\", probabilities)\n",
    "\n",
    "# Filter out bounding boxes with probabilities lower than 0.5\n",
    "filtered_boxes = []\n",
    "filtered_boxes = [\n",
    "    box for box, prob in zip(test_result[0].boxes.xyxy, probabilities) if prob >= 0.4\n",
    "]\n",
    "\n",
    "print(\"Filtered boxes:\", filtered_boxes)\n",
    "\n",
    "coordinates = []\n",
    "\n",
    "for box in filtered_boxes:\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    center_x = (x1 + x2) // 2\n",
    "    center_y = (y1 + y2) // 2\n",
    "\n",
    "    # Normalize coordinates with respect to the origin\n",
    "    normalized_x = center_x - origin[0] # Normalize x-axis\n",
    "    normalized_y = origin[1] - center_y  # Invert y-axis for Cartesian coordinates\n",
    "    coordinates.append((normalized_x, normalized_y))\n",
    "    \n",
    "    text = f\"({normalized_x}, {normalized_y})\"\n",
    "    cv2.circle(original_image, (center_x, center_y), 10, (255, 0, 0), -1)\n",
    "    cv2.putText(original_image, text, (center_x + 10, center_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "print(\"Coordinates of the centers of the bounding boxes:\", coordinates)\n",
    "\n",
    "# Display the image with the centers drawn\n",
    "sv.plot_image(original_image, size=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to detections to .JSON format\n",
    "\n",
    "Cannot be json must be txt or just full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_coordinates = [(label, coord[0], coord[1]) for label, coord in zip(labels, coordinates)]\n",
    "print(\"Object coordinates:\", object_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonschema\n",
    "import json\n",
    "\n",
    "objectschema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"objects\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"label\": {\"type\": \"string\"},\n",
    "                    \"x\": {\"type\": \"number\"},\n",
    "                    \"y\": {\"type\": \"number\"}\n",
    "                },\n",
    "                \"required\": [\"label\", \"x\", \"y\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"objects\"]\n",
    "}\n",
    "\n",
    "# Prepare data in the required format\n",
    "data = {\n",
    "    \"objects\": [\n",
    "        {\"label\": label, \"x\": x, \"y\": y}\n",
    "        for label, x, y in object_coordinates\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Validate the data against the schema\n",
    "jsonschema.validate(instance=data, schema=objectschema)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open(\"object_coordinates.txt\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "    \n",
    "# Print the JSON data\n",
    "print(json.dumps(data, indent=4))\n",
    "\n",
    "print(\"Object coordinates saved to object_coordinates.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=constants.API_KEY)\n",
    "model_name=\"gemini-2.5-pro-exp-03-25\"\n",
    "\n",
    "prompt = \"\"\"You are a network engineer. You are given a .txt file, count the total number of users and estimate the bandwidth needed for each user.\n",
    "            Additionaly replace the entries of the all the object to individual users, for example a bus counts as 3 users, so for every bus in the file you have to create three entries with the same coordinates in the response, but with the label user.\n",
    "            The file contains the coordinates of the objects in the image, and you need to provide the coordinates of each user in the response.\n",
    "        \"\"\"\n",
    "system_instructions = \"Note that a car, a bus or a truck count as thee users a airplane or chopper count as 2 users and a person as a single user.\"\n",
    "file = client.files.upload(file=\"object_coordinates.txt\")\n",
    "print(f\"File uploaded: {file}\")\n",
    "\n",
    "class user(BaseModel):\n",
    "    label: str\n",
    "    x: float\n",
    "    y: float\n",
    "\n",
    "class Model_Output(BaseModel):\n",
    "    total_users: int\n",
    "    bandwidth_per_user: float\n",
    "    users: list[user]\n",
    "    \n",
    "response = client.models.generate_content(\n",
    "    model=model_name,\n",
    "    contents=[file,prompt],\n",
    "    config=genai.types.GenerateContentConfig(\n",
    "        system_instruction=system_instructions,\n",
    "        # temperature=0.5,\n",
    "        #response_mime_type='application/json',\n",
    "        response_schema=Model_Output.model_json_schema(),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai # to use the GenAI API\n",
    "from pydantic import BaseModel # to enforce a .JSON output schema to the model\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import cv2\n",
    "\n",
    "%env API_KEY=AIzaSyDowhUDHpvNvoZE4oruhC6PMEnqhpnPCTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini Models\n",
    "|a|b|c|\n",
    "|-------|-------|-------|\n",
    "|Gemini 2.5 Pro Experimental|**gemini-2.5-pro-exp-03-25**|Audio, images, videos, and text \tText \tEnhanced thinking and reasoning, multimodal understanding, advanced coding, and more|\n",
    "|Gemini 2.0 Flash| **gemini-2.0-flash**\t|Audio, images, videos, and text \tText, images (experimental), and audio (coming soon) \tNext generation features, speed, thinking, realtime streaming, and multimodal generation|\n",
    "|Gemini 2.0 Flash-Lite| **gemini-2.0-flash-lite**|\tAudio, images, videos, and text \tText \tCost efficiency and low latency|\n",
    "|Gemini 1.5 Flash| **gemini-1.5-flash**|\tAudio, images, videos, and text \tText \tFast and versatile performance across a diverse variety of tasks|\n",
    "|Gemini 1.5 Flash-8B| **gemini-1.5-flash-8b** |\tAudio, images, videos, and text  Text \tHigh volume and lower intelligence tasks|\n",
    "|Gemini 1.5 Pro| **gemini-1.5-pro** |\tAudio, images, videos, and text \tText  Complex reasoning tasks requiring more intelligence|\n",
    "|Gemini Embedding| **gemini-embedding-exp** |\tText \tText embeddings \tMeasuring the relatedness of text strings|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and retrieve the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_path = 'Testes-main/Images'\n",
    "\n",
    "image_paths = []\n",
    "\n",
    "for filename in os.listdir(image_dir_path):\n",
    "    if (filename.endswith(\".jpg\") or filename.endswith(\".png\")) and filename.startswith(\"fire\"):\n",
    "        image_path = os.path.join(image_dir_path, filename)\n",
    "        image_paths.append(image_path)\n",
    "        \n",
    "print(f\"Found {len(image_paths)} images in {image_dir_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths.sort()\n",
    "titles=[\n",
    "    os.path.basename(image_path)\n",
    "    for image_path in image_paths[:len(image_paths)]\n",
    "    ]               \n",
    "\n",
    "images = [\n",
    "    cv2.imread(str(image_path))\n",
    "    for image_path in image_paths[:len(image_paths)]\n",
    "    ]\n",
    "\n",
    "sv.plot_images_grid(images=images, titles=titles,grid_size=(3,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO model selection & object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = YOLO('yolo11x.pt')\n",
    "model12 = YOLO('yolo12x.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "titles = []\n",
    "for image_path in image_paths:\n",
    "    test_iamge_path = cv2.imread(str(image_path))\n",
    "    test_iamge_path_resized = cv2.resize(test_iamge_path, (640, 640))\n",
    "    \n",
    "    results.append(model11(test_iamge_path))\n",
    "    titles.append(os.path.basename(image_path)+\" - yolo11\")\n",
    "    results.append(model11(test_iamge_path_resized))\n",
    "    titles.append(os.path.basename(image_path)+\" - yolo11 resized\")\n",
    "    results.append(model12(test_iamge_path))\n",
    "    titles.append(os.path.basename(image_path)+\" - yolo12\")\n",
    "    \n",
    "    \n",
    "    results.append(model12(test_iamge_path_resized))\n",
    "    titles.append(os.path.basename(image_path)+\" - yolo12 resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_images = []\n",
    "for i in range(0, len(results), 4):\n",
    "    comparison_images.append(results[i][0].plot())\n",
    "    comparison_images.append(results[i + 1][0].plot())\n",
    "    comparison_images.append(results[i + 2][0].plot())\n",
    "    comparison_images.append(results[i + 3][0].plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.plot_images_grid(images=comparison_images, titles=titles, grid_size=(5, 4), size=(50, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the image to analyze and extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model11\n",
    "test_iamge_path = image_paths[0]\n",
    "original_image = cv2.imread(test_iamge_path)\n",
    "test_result = results[0]\n",
    "test_image = test_result[0].plot()\n",
    "sv.plot_image(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the origin (0, 0) point at the center of the image\n",
    "origin = (original_image.shape[1] // 2, original_image.shape[0] // 2)\n",
    "\n",
    "# Draw the X-axis\n",
    "cv2.line(original_image, (0, origin[1]), (original_image.shape[1], origin[1]), (0, 255, 5), 2)\n",
    "\n",
    "# Draw the Y-axis\n",
    "cv2.line(original_image, (origin[0], 0), (origin[0], original_image.shape[0]), (0, 255, 5), 2)\n",
    "\n",
    "# Add labels for the axes\n",
    "cv2.putText(original_image, 'X', (original_image.shape[1] - 50, origin[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 5), 2)\n",
    "cv2.putText(original_image, 'Y', (origin[0] + 10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 5), 2)\n",
    "cv2.putText(original_image, '(0,0)', (origin[0] + 10, origin[1] + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 5), 2)\n",
    "\n",
    "# Display the image with the referential\n",
    "sv.plot_image(original_image, size=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [model.names.get(cls.item()) for cls in test_result[0].boxes.cls.int()]  \n",
    "        \n",
    "unique_labels = set(labels)\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "\n",
    "probabilities = test_result[0].boxes.conf\n",
    "print(\"Probabilities:\", probabilities)\n",
    "\n",
    "# Filter out bounding boxes with probabilities lower than 0.5\n",
    "filtered_boxes = []\n",
    "filtered_boxes = [\n",
    "    box for box, prob in zip(test_result[0].boxes.xyxy, probabilities) if prob >= 0.4\n",
    "]\n",
    "\n",
    "print(\"Filtered boxes:\", filtered_boxes)\n",
    "\n",
    "coordinates = []\n",
    "\n",
    "for box in filtered_boxes:\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    center_x = (x1 + x2) // 2\n",
    "    center_y = (y1 + y2) // 2\n",
    "\n",
    "    # Normalize coordinates with respect to the origin\n",
    "    normalized_x = center_x - origin[0] # Normalize x-axis\n",
    "    normalized_y = origin[1] - center_y  # Invert y-axis for Cartesian coordinates\n",
    "    coordinates.append((normalized_x, normalized_y))\n",
    "    \n",
    "    text = f\"({normalized_x}, {normalized_y})\"\n",
    "    cv2.circle(original_image, (center_x, center_y), 10, (255, 0, 0), -1)\n",
    "    cv2.putText(original_image, text, (center_x + 10, center_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "print(\"Coordinates of the centers of the bounding boxes:\", coordinates)\n",
    "\n",
    "# Display the image with the centers drawn\n",
    "sv.plot_image(original_image, size=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to detections to .JSON format\n",
    "\n",
    "Cannot be json must be txt or just full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_coordinates = [(label, coord[0], coord[1]) for label, coord in zip(labels, coordinates)]\n",
    "print(\"Object coordinates:\", object_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonschema\n",
    "import json\n",
    "\n",
    "objectschema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"objects\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"label\": {\"type\": \"string\"},\n",
    "                    \"x\": {\"type\": \"number\"},\n",
    "                    \"y\": {\"type\": \"number\"}\n",
    "                },\n",
    "                \"required\": [\"label\", \"x\", \"y\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"objects\"]\n",
    "}\n",
    "\n",
    "# Prepare data in the required format\n",
    "data = {\n",
    "    \"objects\": [\n",
    "        {\"label\": label, \"x\": x, \"y\": y}\n",
    "        for label, x, y in object_coordinates\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Validate the data against the schema\n",
    "jsonschema.validate(instance=data, schema=objectschema)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open(\"object_coordinates.txt\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "    \n",
    "# Print the JSON data\n",
    "print(json.dumps(data, indent=4))\n",
    "\n",
    "print(\"Object coordinates saved to object_coordinates.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=os.getenv(\"API_KEY\"))\n",
    "model_name=\"gemini-2.5-pro-exp-03-25\"\n",
    "\n",
    "prompt = \"\"\"You are a network engineer. You are given a .txt file, count the total number of users and estimate the bandwidth needed for each user.\n",
    "            Additionaly replace the entries of the all the object to individual users, for example a bus counts as 3 users, so for every bus in the file you have to create three entries with the same coordinates in the response, but with the label user.\n",
    "            The file contains the coordinates of the objects in the image, and you need to provide the coordinates of each user in the response.\n",
    "        \"\"\"\n",
    "system_instructions = \"Note that a car, a bus or a truck count as thee users a airplane or chopper count as 2 users and a person as a single user.\"\n",
    "file = client.files.upload(file=\"object_coordinates.txt\")\n",
    "print(f\"File uploaded: {file}\")\n",
    "\n",
    "class user(BaseModel):\n",
    "    label: str\n",
    "    x: float\n",
    "    y: float\n",
    "\n",
    "class Model_Output(BaseModel):\n",
    "    total_users: int\n",
    "    bandwidth_per_user: float\n",
    "    users: list[user]\n",
    "    \n",
    "response = client.models.generate_content(\n",
    "    model=model_name,\n",
    "    contents=[file,prompt],\n",
    "    config=genai.types.GenerateContentConfig(\n",
    "        system_instruction=system_instructions,\n",
    "        # temperature=0.5,\n",
    "        #response_mime_type='application/json',\n",
    "        response_schema=Model_Output.model_json_schema(),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
